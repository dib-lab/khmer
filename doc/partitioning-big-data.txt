=========================================
Partitioning large data sets (50m+ reads)
=========================================

We've successfully used khmer to partition extremely large data sets.
These are challenging because of the data size, the memory
constraints, and the time involved.  You'll probably need 60-80gb of
RAM to follow this process; we usually just rent EC2 machines at
http://aws.amazon.com/.

Here's a rough guide.

Step 1a: Load the reads into a hashtable and tagset
==================================================

Load all of the k-mers in all of the reads in memory.

This builds the basic data structure (a fixed-size hash table), which
we can traverse as a probabilistic de Bruijn graph; and a set of
k-mers that are tagged at a minimum density within that graph (the
tag set).  In terms of memory consumption, the hashtable is fixed size
and the tag set is linear with the number of reads (one tagged k-mer
per read).

Implemented in do-th-subset-save.py (first part).

Make the hashtable as big as possible.  Use large K.

Note: hashtable occupancy should be less than 50%, rule of thumb.

Optional: save the hashtable and tagset; then you won't need to parse
again.  See ht.save(filename) and ht.save_tagset(filename).

Example: ::

  %% python ./scripts/do-th-subset-save.py data/25k.fa

Step 1b: Do subset partitioning
==============================

Break the set of all tags into smaller subsets (1-5m in size is a good
low-memory choice) and do a tabu search from each tag to all possible
connected tags (stopping at the first connected tag in each
path). Then label all connected tags with a partition ID. These
subsets can be evaluated in parallel, then saved to disk when finished.

Implemented in do-th-subset-save.py (second part).

The smaller the subset size, the less memory used and the more threads
can be used to partition subsets in parallel.  We would suggest
using a subset size of 1m with 8 threads, which is small enough to be
evaluated and saved quickly, without using much memory.

After subset partitioning, neither the hashtable nor the original tagset
is needed.

Step 2: Merge subsets
=====================

Do a pairwise merge of as many subsets as possible.  This is memory-
and CPU- and disk-intensive.  Keep the number of parallel threads as
high as possible without running out of memory.

Implemented in do-subset-merge.py.

Repeat step #3 until you can't do any more merges.

Step 3: Filter the subsets
==========================

Now do a filtering of partitions that are less than L in size in all
of the subsets.  I use L=3.

@@ wax philosophic

Step 4: Output the reads with their partitions
==============================================

Next, load/merge all the subsets (either as in step 4, or in memory),
and run through the entire original read file. Output any read that
has been assigned a partition.

Implemented in do-th-subset-save.py.

Step 5: Group the sequences into separate groups, based on partition size
=========================================================================

Finally, we want to produce some files feed to the assembler.  While
we *could* assemble each partition individually, in practice
there will 10s of thousands to millions of partitions in a single large
data set.  So instead we want to group them.

The strategy we use is to go through the partitions, counting the number
of reads in each; then order the partitions by size, smallest to largest;
then output all the reads into a series of group files, each of a particular
minimum size (containing a set of reads).  (There's no particular logic
as to why to do it this way; you could just as easily randomly assign
partitions to groups.)

Implemented in extract-partitions.py.
